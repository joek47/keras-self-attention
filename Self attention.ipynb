{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 Aug 2019: Answer to \n",
    "https://stackoverflow.com/questions/57438806/how-to-add-an-attention-layer-along-with-a-bi-lstm-layer-in-keras-sequential-m\n",
    "\n",
    "From https://github.com/CyberZHG/keras-self-attention/issues/12:\n",
    "- The output of the attention layer in this repo has the same shape as the input (therefore it is 3 dimensional in this case). What you need maybe something like this one: SeqWeightedAttention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras_self_attention import SeqSelfAttention, SeqWeightedAttention\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "\n",
    "# data\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen= maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# model \n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 64)          16640     \n",
      "_________________________________________________________________\n",
      "seq_weighted_attention_1 (Se (None, 64)                65        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 336,770\n",
      "Trainable params: 336,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "20000/20000 [==============================] - 275s 14ms/step - loss: 0.5686 - acc: 0.7017 - val_loss: 0.4345 - val_acc: 0.8276\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "# model.add( Embedding(max_features, 32,  mask_zero=True))\n",
    "model.add( Embedding(max_features, 32))\n",
    "model.add(Bidirectional( LSTM(32, return_sequences=True)))\n",
    "# add an attention layer\n",
    "\n",
    "# model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "model.add(SeqWeightedAttention())\n",
    "\n",
    "model.add( Dense(1, activation='sigmoid') )\n",
    "\n",
    "# compile and fit\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=1, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-self-attention\n",
      "  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n",
      "Requirement already satisfied: numpy in /home/lam/anaconda2/lib/python2.7/site-packages (from keras-self-attention)\n",
      "Collecting Keras (from keras-self-attention)\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "\u001b[K    100% |████████████████████████████████| 317kB 1.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from Keras->keras-self-attention)\n",
      "  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 1.6MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/lam/anaconda2/lib/python2.7/site-packages (from Keras->keras-self-attention)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/lam/anaconda2/lib/python2.7/site-packages (from Keras->keras-self-attention)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/lam/anaconda2/lib/python2.7/site-packages (from Keras->keras-self-attention)\n",
      "Collecting keras-applications>=1.0.6 (from Keras->keras-self-attention)\n",
      "  Downloading https://files.pythonhosted.org/packages/21/56/4bcec5a8d9503a87e58e814c4e32ac2b32c37c685672c30bc8c54c6e478a/Keras_Applications-1.0.8.tar.gz (289kB)\n",
      "\u001b[K    100% |████████████████████████████████| 296kB 1.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /home/lam/anaconda2/lib/python2.7/site-packages (from Keras->keras-self-attention)\n",
      "Building wheels for collected packages: keras-self-attention, keras-applications\n",
      "  Running setup.py bdist_wheel for keras-self-attention ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/lam/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n",
      "  Running setup.py bdist_wheel for keras-applications ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/lam/.cache/pip/wheels/dd/f2/5d/2689b5547f32c4e258c3b7ccbe7f1d0f2afbb84fb01e830792\n",
      "Successfully built keras-self-attention keras-applications\n",
      "Installing collected packages: keras-preprocessing, keras-applications, Keras, keras-self-attention\n",
      "Successfully installed Keras-2.2.4 keras-applications-1.0.8 keras-preprocessing-1.1.0 keras-self-attention-0.41.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install keras-self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm attention",
   "language": "python",
   "name": "lstm_att"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
